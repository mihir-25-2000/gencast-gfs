{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af831406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96999bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "global_list = []\n",
    "\n",
    "def log2(x):\n",
    "  global_list.append(x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36645eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printed x: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_with_print(x):\n",
    "  print(\"printed x:\", x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2_with_print)(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced498b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jax import random, grad, jit\n",
    "\n",
    "# Generate toy 2D data (two Gaussian blobs) -----\n",
    "key = random.PRNGKey(0)\n",
    "n_per_class = 250\n",
    "\n",
    "key1, key2, key3 = random.split(key, 3)\n",
    "mean0 = jnp.array([-1.0, -1.0])\n",
    "mean1 = jnp.array([+1.0, +1.0])\n",
    "\n",
    "X0 = mean0 + 0.6 * random.normal(key1, (n_per_class, 2))\n",
    "X1 = mean1 + 0.6 * random.normal(key2, (n_per_class, 2))\n",
    "X = jnp.concatenate([X0, X1], axis=0)                             # (N, 2)\n",
    "y = jnp.concatenate([jnp.zeros(n_per_class), jnp.ones(n_per_class)])  # labels in {0,1}\n",
    "\n",
    "# Shuffle\n",
    "perm = random.permutation(key3, X.shape[0])\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# -- Model & loss -----\n",
    "def logits(params, X):\n",
    "    w, b = params\n",
    "    return X @ w + b  # (N,)\n",
    "\n",
    "# Binary cross-entropy via softplus for stability:\n",
    "# BCE = mean( softplus(logit) - y*logit )\n",
    "def loss_fn(params, X, y, l2=0.0):\n",
    "    z = logits(params, X)\n",
    "    data_loss = jnp.mean(jnp.logaddexp(0.0, z) - y * z)\n",
    "    reg = 0.5 * l2 * (jnp.sum(params[0] ** 2) + params[1] ** 2)\n",
    "    return data_loss + reg\n",
    "\n",
    "# --- Gradients (jax.grad) -----\n",
    "loss_grad = grad(loss_fn)\n",
    "\n",
    "# ---- Gradient descent loop -----\n",
    "def accuracy(params, X, y):\n",
    "    z = logits(params, X)\n",
    "    yhat = (z > 0).astype(y.dtype)\n",
    "    return jnp.mean(yhat == y)\n",
    "\n",
    "@jit\n",
    "def step(params, X, y, lr, l2):\n",
    "    g_w, g_b = loss_grad(params, X, y, l2)\n",
    "    w, b = params\n",
    "    w = w - lr * g_w\n",
    "    b = b - lr * g_b\n",
    "    return (w, b)\n",
    "\n",
    "# Init\n",
    "params = (jnp.zeros(2), 0.0)   # (w, b)\n",
    "lr = 0.5\n",
    "l2 = 1e-3\n",
    "num_steps = 400\n",
    "\n",
    "for t in range(1, num_steps + 1):\n",
    "    params = step(params, X, y, lr, l2)\n",
    "    if t % 50 == 0:\n",
    "        current_loss = loss_fn(params, X, y, l2)\n",
    "        acc = accuracy(params, X, y)\n",
    "        print(f\"step {t:3d} | loss {current_loss:.4f} | acc {acc*100:.1f}%\")\n",
    "\n",
    "w, b = params\n",
    "print(\"\\nFinal params:\")\n",
    "print(\"w:\", w, \" b:\", float(b))\n",
    "print(\"Final train accuracy:\", float(accuracy(params, X, y)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
